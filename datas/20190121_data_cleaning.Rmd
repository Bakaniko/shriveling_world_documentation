---
title: "Shriveling World</br>Cleaning cities data"
author:
- Nicolas Roelandt
- IFSTTAR/AME
date: "21/01/2019"
output:
  html_document:
    df_print: paged
    toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Goal:

Remove from a dataset of cities, the cities that are too clause to eah other.

For example, Islamabad and Rawalpindi (Pakistan) who are distant from less than 10 kilometers.
There is no need to keep them both as the data sample works for the whole world.

Steps:

1. Load the data and create geometries
2. Create pairs of close cities
3. Identifie the biggest city, remove the smallest and add its population to the biggest
4. Export the data


# Loading needed packages
This script require some spatial packages who use geospatial C++ core libraries. You have to download and to install
them before installing {sf}.

You can find information on how to install it in Ubuntu 18.04 there:

https://rtask.thinkr.fr/blog/installation-of-r-3-5-on-ubuntu-18-04-lts-and-tips-for-spatial-packages/

```{r load_libraries, collapse = TRUE}
library(tidyverse)
library(sf)
library(mapview) # may need to install webshot::install_phantomjs()
library(units)
library(lwgeom)
library(reshape2)
library(here)
set_here()

```

# Load the data

Datasource is 2 CSV files:

- cities.csv with WGS84 coordinates
- population.csv : the population for the correspondig cities

The csv files are loaded then 

```{r load_data_source}
cities <- read_csv(here("cities.csv"), col_types = cols(radius = col_number(), 
    yearHST = col_integer(), yearMotorway = col_integer()))
head(cities)

population <- read_csv(here("population.csv"))
```

The line 1371 is the only one to have a radius. Probably a typo.

So we have 10 columns, we will need the **latitude** and **longitude** ones to create points.

# Create geometries

```{r Create geometry}
cities_df = st_as_sf(cities, coords = c("longitude", "latitude"), crs = 4326)
head(cities_df)
```
## Check Geometry 
### Plot the geometries

```{r View_geometries}

ggplot(cities_df) + 
  geom_sf()+
  ggtitle("Cities positions")
```

### Using mapview

This will allow to display a basemap so we can check if cities are correctly placed.


```{r map_using_mapview, eval=TRUE}
m <- mapview(cities_df, map.types="CartoDB.Positron")
#mapshot(m, file = paste0(getwd(), "figures/cities_example.png"))

#mapshot(m, file = paste0(here("example", "datas","figures","cities_example.png")))
m
```



```{r, echo=FALSE, eval=FALSE}
# This is dummy data for testing.
# Create a tribble from the data

df <- tibble::tribble(
   ~index,             ~city,     ~lat,    ~lon,
  1172,           "Zaria", 11.11128,   7.7227,
  1173,            "Oslo", 59.91273, 10.74609,
  1174, "Masqat (Muscat)", 23.61387,  58.5922,
  1175,      "Bahawalpur",     29.4, 71.68333,
  1181,       "Islamabad", 33.70351,73.059373,
  1194,      "Rawalpindi",     33.6,73.0666667
  
  )
df

cities_df_simplified <- st_as_sf(df, coords = c("lon", "lat"), crs = 4326)
```

Constants:
Here we will fix the limit under which we want to find close cities.

```{r constants}
buffer_size = set_units(20000, 'm') # 20 km
```

## Simplification of the dataset

We don't need all the columns now so we can simplified the dataset.

```{r, df_simplification}
cities_df_simplified <- cities_df %>%
  select("cityCode")

head(cities_df_simplified)
```

## Distance matrix creation

As WGS84 is in spherical geometry, we will need to use the *st_distance()* function with the capabilities provided by {lwgeom}.

Source: https://stackoverflow.com/questions/54294484/rstats-how-to-convert-kilometers-into-arc-degrees-in-order-to-create-buffers-w/54296072#54296072

```{r, distance_matrix}
dist_mat <- st_distance(cities_df_simplified)

# Create a Truth matrix when 0 < distance < 200000 meters
# 

truth_matrix <- dist_mat < buffer_size & dist_mat > set_units(0,'m')


# Put cities index as row and column names
dimnames(truth_matrix) <- list(cities_df$cityCode, cities_df$cityCode)

# fusion de la matrice pour créer une colonne pour chaque identifiant
# les valeur sont filtrées pour ne garder que les valeurs posiitives.
# source: https://stackoverflow.com/questions/32024620/subset-data-frame-to-get-rowname-colname-and-value

extraction <- subset(melt(truth_matrix), value ==1)
head(extraction)
```

*st_distance()* returns a matrix of distances between the cities. 
Then we check if this distance is between 0 and 20000 meters (TRUE) or not (FALSE) 

Then we concatenate back the city codes as columns and row names.
The matrix is then melted to 2 columns of city codes and the distance between those 2 cities.
A subset of true values (== 1) is created. 


*Var1* and *Var2* are Origin and Destination city codes.


```{r count_unique, eval=FALSE, echo=FALSE}
# Test if there is no singleton or loose cities between the 2 lists
# How many unique cities in pairs:
toto <-sort(unique(extraction$Var1))
tata <-sort(unique(extraction$Var2))
toto == tata # not evaluated // should be all TRUE

```


# Create pairs 
The pairs are created from the extraction, by creating a new id unique ID *unique_order* based on the size of the city codes.
The doubles are cleared.

```{r create_pairs}

vars <- c(cityCodeV1 = "Var1", cityCodeV2 ="Var2") # rename columns
pairs <- extraction %>%
  rowwise() %>%
  mutate(unique_order =
           paste(min(Var1, Var2),
                 max(Var1, Var2), sep = '-')) %>%
  distinct(unique_order, .keep_all = TRUE) %>%
  rename(!!vars)

```

To the pairs are join data from the origin city like th geometry this is done to visualize their position on the map.


```{r Add_geometry}
pairs_geom <-  pairs %>% 
  left_join(cities_df, by =  c("cityCodeV1" = "cityCode")) %>%
  st_as_sf()
head(pairs_geom)

m <- mapview(pairs_geom, map.types="CartoDB.Positron")
#mapshot(m, file = paste0(getwd(), "figures/pairs_cities.png"))
m
```

# Order by population
As we want to remove the smallest city and add its population to the biggest, we first need to had population of each city using its city code.

```{r population_order}
# Create a simple df with only cityCode and 2015 population
pop2015 <- population %>%
  select(cityCode, pop2015)

pairs_pop <- pairs_geom %>%
  left_join(pop2015, by = c("cityCodeV1" = "cityCode")) %>%
  rename(v1_pop2015 = pop2015) %>%
  left_join(pop2015, by =c("cityCodeV2" = "cityCode")) %>%
  rename(v2_pop2015 = pop2015)
head(pairs_pop)
```

Then we create a new data frame where we keep the citycode of the biggest city and the sum of the population of the 2 cities.

It was first created with a for loop to test the algorithme then implemented using the function *transmute()* from
the package {dplyr} for educationnal reasons.

## With a loop

```{r citylist_for_loop, eval= FALSE}
## Keep as an example, does the same as the next block code
to_clean_df <- setNames(data.frame(matrix(ncol = 2, nrow = dim(pairs_pop)[1])),c("to_keep", "to_keep_pop"))
# For each row, determine which city is the biggest and adds the two cities population
for (i  in 1:dim(pairs_pop)[1]) {
  
  
  if(pairs_pop$v1_pop2015[i] > pairs_pop$v2_pop2015[i]){
    to_clean_df$to_keep[i] = pairs_pop$cityCodeV1[i]
    to_clean_df$to_keep_pop[i] = pairs_pop$v1_pop2015[i] + pairs_pop$v2_pop2015[i]
    
  } 
  else 
  {
    to_clean_df$to_keep[i] = pairs_pop$cityCodeV2[i]
    to_clean_df$to_keep_pop[i] = pairs_pop$v1_pop2015[i] + pairs_pop$v2_pop2015[i]
  }
  
}
to_clean_df 
```

## With *transmute()*

```{r cities_to_remove_qith_transmute}
clean_df <- transmute(pairs_pop,
              to_keep = if_else(v1_pop2015 > v2_pop2015, cityCodeV1, cityCodeV2),
              to_keep_pop = v1_pop2015 + v2_pop2015)

citycode_to_keep <- unique(clean_df$to_keep) # 48 cities
head(citycode_to_keep)
```

At the end of this, we have extracted from the pairs a list of cities to keep.

# Formating and writing output

Going back to the *extraction* dataframe, we create a dataframe of rejected cities with the *antijoin()* function.

From the dataframe of all the cities we perform an antijoin again with the rejected cities to keep only the cities that are not rejected. 

```{r formating_output}
# list of cities to keep
## all cities - pairs + cities to keep

# Many cities to delete 
length(unique(extraction$Var1)) - length(citycode_to_keep )

### list of rejected cities
rejected_cities <- extraction %>% # from all cities of the truth matrix
  distinct(Var1) %>% # keep unique citycode (Var1 == Var2)
  anti_join(clean_df, by = c("Var1" = "to_keep")) %>% # remove cities we want to keep
  rename(cityCode = Var1) # change column name for next step


### list of all cities without rejected
# cities antijoin with rejected
# list of citycode

## filter origal data (without geom) to keep file structure
cities_out <- cities %>% 
  anti_join(rejected_cities, by = "cityCode") # remove rejected cities

# reduced data set for tests
china_out <- cities_out %>%
  filter(countryName == "China")

dim(cities_out) # 1638 cities, 43 out, 1681 total
##
```

Finally we write the new csv files.

```{r, writing_output}
write_csv(cities_out, "to_drop_in_app/cleaned_cities.csv")
write_csv(china_out, "to_drop_in_app_sample/cleaned_cities_China.csv")
```



# Issues
## Distance criteria
We choose a 20 km distance, maybe there is something more robust to get that distance.


## Cleaning Population and transport networks
The Population and network files still contains data from removed cities.
It will be good to clean them as well.
